{"cells":[{"cell_type":"markdown","metadata":{"id":"EeUWdEX3RzPf"},"source":["# Advanced Image Processing (TM11005)\n","## *Week 1: Segmentation, Exercise 3: Deep Learning*\n","*Author: Karin van Garderen*\n","### Spleen 3D segmentation with MONAI\n","\n","This exercise was adapted from the [MONAI tutorials on Spleen segmentation](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb).\n","MONAI is an open-source framework for deep learning in medical imaging. It contains a wide range of popular network architectures, loss functions and functions for loading and transforming images. This tutorial will make you familiar with the basics of deep learning for medical image segmentation using MONAI.\n","\n","The Spleen dataset can be downloaded from http://medicaldecathlon.com/.\n","\n","![spleen](http://medicaldecathlon.com/img/spleen0.png)\n","\n","Target: Spleen  \n","Modality: CT  \n","Size: 61 3D volumes (41 Training + 20 Testing)  \n","Source: Memorial Sloan Kettering Cancer Center \n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MStarmans91/AIP-exercises/blob/cnn_exercise/Week1_Segmentation/Week1.4_CNN/spleen_segmentation_3d.ipynb)\n","\n","## **Before you start**\n","\n","The CNN's used in this tutorial run much faster with the right hardware. It is advised to run this notebook on Google Colab with GPU accelaration. Make sure you turn on GPU acceleration on Colab by going to Runtime -> Change Runtime type\n","\n","## **Handing in your answers**\n","For each exercise, you have to hand in answers to questions, and\n","for some also the code. Hence please only hand in two files in total\n","for this exercise set:\n","\n","- Code.py (or .ipynb): a Python script / jupyter notebook.\n","- Answers.docx (or .PDF): a text file with the answers to the\n","questions (plots, text, ...)."]},{"cell_type":"code","source":["# Copyright 2020 MONAI Consortium\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"metadata":{"id":"jb2XpvOMlOkp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SvCPkYg3RzPk"},"source":["## Setup environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQONHpdXRzPl"},"outputs":[],"source":["## No need to edit this\n","!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\"\n","!python -c \"import matplotlib\" || pip install -q matplotlib\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"szCIYvwBRzPn"},"source":["## Setup imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E604crt5RzPm","tags":[]},"outputs":[],"source":["## No need to edit this\n","\n","from monai.utils import first, set_determinism\n","from monai.transforms import (\n","    AsDiscrete,\n","    AsDiscreted,\n","    EnsureChannelFirstd,\n","    Compose,\n","    CropForegroundd,\n","    LoadImaged,\n","    Orientationd,\n","    RandCropByPosNegLabeld,\n","    SaveImaged,\n","    ScaleIntensityRanged,\n","    Spacingd,\n","    EnsureTyped,\n","    EnsureType,\n","    Invertd,\n","    SqueezeDimd,\n","    CenterSpatialCropd,\n","    DivisiblePadd\n",")\n","from monai.handlers.utils import from_engine\n","from monai.networks.nets import UNet\n","from monai.networks.layers import Norm\n","from monai.metrics import DiceMetric\n","from monai.losses import DiceLoss\n","from monai.inferers import sliding_window_inference\n","from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n","from monai.config import print_config\n","from monai.apps import download_and_extract\n","import torch\n","import matplotlib.pyplot as plt\n","import tempfile\n","import shutil\n","import os\n","import glob\n","print_config()"]},{"cell_type":"markdown","metadata":{"id":"y_2ZQZIZRzPo"},"source":["## Setup data directory\n","\n","You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n","This allows you to save results and reuse downloads.  \n","If not specified a temporary directory will be used."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-Cc50xLRzPp","tags":[]},"outputs":[],"source":["## No need to edit this\n","\n","directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n","root_dir = tempfile.mkdtemp() if directory is None else directory\n","print(root_dir)"]},{"cell_type":"markdown","metadata":{"id":"f1zasHVCRzPp"},"source":["## Download dataset\n","\n","Downloads and extracts the dataset.  \n","The dataset comes from http://medicaldecathlon.com/."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKy64EwCRzPq","tags":[]},"outputs":[],"source":["## No need to edit this\n","\n","resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n","md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n","\n","compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n","data_dir = os.path.join(root_dir, \"Task09_Spleen\")\n","if not os.path.exists(data_dir):\n","    download_and_extract(resource, compressed_file, root_dir, md5)"]},{"cell_type":"markdown","metadata":{"id":"l1JIGejtRzPr"},"source":["## Set MSD Spleen dataset path\n","First, we will set up the dataset and split split it in a training and test set. For each patient, we have a CT image and a label image in the form of two files. We will store them as a dictionary per patient, referencing the files we just downloaded. The last 9 patients will form the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3dVa8JNORzPs"},"outputs":[],"source":["## No need to edit this\n","\n","train_images = sorted(\n","    glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n","train_labels = sorted(\n","    glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n","data_dicts = [\n","    {\"image\": image_name, \"label\": label_name}\n","    for image_name, label_name in zip(train_images, train_labels)\n","]\n","train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n","print(data_dicts)"]},{"cell_type":"markdown","metadata":{"id":"9_beRwyNRzPt"},"source":["## Set deterministic training for reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tSQf6WcRzPt"},"outputs":[],"source":["set_determinism(seed=0)"]},{"cell_type":"markdown","metadata":{"id":"lNwr9PgoRzPu"},"source":["## Setup transforms for training and validation\n","\n","Here we use several transforms to make sure the data is optimal for training. \n","The following transforms will be applied:\n","1. `LoadImaged` loads the spleen CT images and labels from NIfTI format files.\n","1. `AddChanneld` as the original data is 3D, add 1 dimension to serve as 'channels', even though this dataset only has one channel.\n","1. `Orientationd` unifies the data orientation based on the affine matrix defined in the NIfTI format.\n","1. `Spacingd` adjusts the spacing by `pixdim=(1.5, 1.5, 2.)` based on the affine matrix.\n","1. `ScaleIntensityRanged` extracts intensity range [-57, 164] and scales to [0, 1].\n","1. `CropForegroundd` removes all zero borders to focus on the valid body area of the images and labels.\n","1. `RandCropByPosNegLabeld` randomly crop patch samples from big image based on pos / neg ratio.  \n","The image centers of negative samples must be in valid body area.\n","1. `EnsureTyped` converts the numpy array to PyTorch Tensor for further steps.\n","\n","By working with the dictionary format, we can make sure that MONAI performs these operations on both the 'image' and 'label' file. The transforms are also applied to the validation set, except here balanced random cropping is not applied."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGjx4syTRzPu"},"outputs":[],"source":["## No need to edit this for now, unless you are working on exercise 3\n","\n","train_transforms = Compose(\n","    [\n","        LoadImaged(keys=[\"image\", \"label\"]),\n","        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n","        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n","        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n","            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n","        ScaleIntensityRanged(\n","            keys=[\"image\"], a_min=-57, a_max=164,\n","            b_min=0.0, b_max=1.0, clip=True,\n","        ),\n","        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n","        RandCropByPosNegLabeld(\n","            keys=[\"image\", \"label\"],\n","            label_key=\"label\",\n","            spatial_size=(96, 96, 96),\n","            pos=1,\n","            neg=1,\n","            num_samples=4,\n","            image_key=\"image\",\n","            image_threshold=0,\n","        ),\n","        EnsureTyped(keys=[\"image\", \"label\"]),\n","    ]\n",")\n","val_transforms = Compose(\n","    [\n","        LoadImaged(keys=[\"image\", \"label\"]),\n","        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n","        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n","        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n","            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n","        ScaleIntensityRanged(\n","            keys=[\"image\"], a_min=-57, a_max=164,\n","            b_min=0.0, b_max=1.0, clip=True,\n","        ),\n","        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n","        EnsureTyped(keys=[\"image\", \"label\"]),\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{"id":"ZITBDTZzRzPv"},"source":["## Check transforms in DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQ5bZp1tRzPv","tags":[]},"outputs":[],"source":["## No need to edit this\n","\n","check_ds = Dataset(data=val_files, transform=val_transforms)\n","check_loader = DataLoader(check_ds, batch_size=1)\n","check_data = first(check_loader)\n","image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n","print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n","# plot the slice [:, :, 80]\n","plt.figure(\"check\", (12, 6))\n","plt.subplot(1, 2, 1)\n","plt.title(\"image\")\n","plt.imshow(image[:, :, 80], cmap=\"gray\")\n","plt.subplot(1, 2, 2)\n","plt.title(\"label\")\n","plt.imshow(label[:, :, 80])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HsJi8b2SRzPv"},"source":["## Define CacheDataset and DataLoader for training and validation\n","\n","Here we use CacheDataset to accelerate training and validation process, it's 10x faster than the regular Dataset.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvA1XatXRzPw","tags":[]},"outputs":[],"source":["## No need to edit this\n","\n","train_ds = CacheDataset(\n","    data=train_files, transform=train_transforms,\n","    cache_rate=1.0, num_workers=2)\n","# train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n","\n","# use batch_size=2 to load images and use RandCropByPosNegLabeld\n","# to generate 4 x 4 images for network training\n","train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=2)\n","\n","val_ds = CacheDataset(\n","    data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=2)\n","# val_ds = Dataset(data=val_files, transform=val_transforms)\n","val_loader = DataLoader(val_ds, batch_size=1, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"d5IYIELpRzPx"},"source":["## Exercise 1: execute a PyTorch training process and track the loss\n","\n","- **Hand-In Code**: The training loop edited below.\n","\n","\n","This is the main training loop. For *n* epochs, we will pass the data through the model and optimize the weights using the loss function. The main loop is already there, but your task is to write the code for the actual training. \n","Additionally, we would like to compute the loss on the validation set in order to track whether the model is overfitting.\n","\n","\n","We start by using a 3D Unet, which we keep small enough to train it quickly.\n","We use PyTorch for backpropagation and optimization of the network, so you can find all the information you need in their documentation. Specifically: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#optimizing-the-model-parameters\n","\n","To give you a hint, you will need to implement the following steps (except in a different order):\n","- Backpropagation of the loss through the network\n","- Pass the data through the model to get the output\n","- Take one step using the optimizer\n","- Compute the loss\n","- Set the gradients to zero\n","- Compute the loss for the validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkyGI4V8RzPx","scrolled":true,"tags":[]},"outputs":[],"source":["## Program your part in the indicated spaces\n","\n","# create UNet, DiceLoss and Adam optimizer\n","device = torch.device(\"cuda:0\")\n","model = UNet(\n","    spatial_dims=3,\n","    in_channels=1,\n","    out_channels=2,\n","    channels=(4, 16, 32, 64),\n","    strides=(2, 2, 2),\n","    num_res_units=2,\n","    norm=Norm.BATCH,\n",").to(device)\n","loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n","optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n","dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n","\n","## Initialize lists to track metrics, set number of epochs and interval for validation.\n","max_epochs = 10\n","val_interval = 1\n","epoch_loss_values = []\n","val_loss_values = []\n","metric_values = []\n","## Transforms to use after prediction\n","post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=2)])\n","post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n","\n","for epoch in range(max_epochs):\n","    print(\"-\" * 10)\n","    print(f\"epoch {epoch + 1}/{max_epochs}\")\n","    model.train()\n","    epoch_loss = 0\n","    epoch_val_loss = 0\n","    step = 0\n","    for batch_data in train_loader:\n","        step += 1\n","\n","        ## Transfer data to the GPU\n","        inputs, labels = (\n","            batch_data[\"image\"].to(device),\n","            batch_data[\"label\"].to(device),\n","        )\n","\n","        #### START PROGRAMMING HERE\n","        \n","        loss = ..\n","        #### END PROGRAMMING HERE\n","\n","        epoch_loss += loss.item()\n","        print(\n","            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n","            f\"train_loss: {loss.item():.4f}\")\n","    epoch_loss /= step\n","    epoch_loss_values.append(epoch_loss)\n","    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n","\n","    ## For monitoring, compute the performance on the validation set\n","    if (epoch + 1) % val_interval == 0:\n","        model.eval()\n","        with torch.no_grad():\n","            for val_data in val_loader:\n","                val_inputs, val_labels = (\n","                    val_data[\"image\"].to(device),\n","                    val_data[\"label\"].to(device),\n","                )\n","                roi_size = (160, 160, 160)\n","                sw_batch_size = 4\n","\n","                ## Use 'sliding window' technique to achieve a prediction for the entire image\n","                val_outputs = sliding_window_inference(\n","                    val_inputs, roi_size, sw_batch_size, model)\n","                \n","\n","                ###### START PROGRAMMING HERE ######\n","                \n","                epoch_val_loss = ...\n","                #### END PROGRAMMING HERE\n","\n","                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n","                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n","                # compute metric for current iteration\n","                dice_metric(y_pred=val_outputs, y=val_labels)\n","\n","            # aggregate the final mean dice result\n","            metric = dice_metric.aggregate().item()\n","            # reset the status for next validation round\n","            dice_metric.reset()\n","\n","            epoch_val_loss /= step\n","            val_loss_values.append(epoch_val_loss)\n","            \n","            print(f'Epoch validation loss: {epoch_val_loss}')\n","\n","            metric_values.append(metric)"]},{"cell_type":"markdown","metadata":{"id":"7IXP4SDERzPy"},"source":["## Exercise 2: Plot the loss over 10 epochs\n","\n","Make sure your run the training loop above for 10 epochs, and plot the loss on train and validation set below. Also plot the Dice metric on the validation set.\n","\n","- **Hand-In Answers**: The generated plots and answers to the following questions.\n","\n","1. **Question:** Are the loss on the training and validation set similar? Which is higher? Name **two** reasons why one would be higher than the other.\n","\n","2. **Question:** Is the Dice metric the same as the Dice loss? If not, why are they different?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZDyV9ltRzPy"},"outputs":[],"source":["### Program here\n"]},{"cell_type":"markdown","metadata":{"id":"USmt6rnnRzPy"},"source":["## Check model output with the input image and label\n","\n","Plot the results on the validation set. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLw-x2bnRzPz"},"outputs":[],"source":["### No need to edit this.\n","\n","model.eval()\n","with torch.no_grad():\n","    for i, val_data in enumerate(val_loader):\n","        roi_size = (160, 160, 160)\n","        sw_batch_size = 4\n","        val_outputs = sliding_window_inference(\n","            val_data[\"image\"].to(device), roi_size, sw_batch_size, model\n","        )\n","\n","        ## Start programming here\n","        # plot the slice [:, :, 80]\n","        plt.figure(\"check\", (18, 6))\n","        plt.subplot(1, 3, 1)\n","        plt.title(f\"image {i}\")\n","        plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n","        plt.subplot(1, 3, 2)\n","        plt.title(f\"label {i}\")\n","        plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n","        plt.subplot(1, 3, 3)\n","        plt.title(f\"output {i}\")\n","        plt.imshow(torch.argmax(\n","            val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n","        plt.show()\n","        if i == 2:\n","            break"]},{"cell_type":"markdown","source":["## Exercise 3: Experiment\n","\n","From the results so far it is clear that there is room for improvement. Pick one of the following changes to the process and run an experiment to see how it affects the performance.\n","\n","\n","*   Make the UNet larger (See https://docs.monai.io/en/stable/networks.html#unet for the documentation)\n","*   Change the balance between foreground and background patches during training (See https://docs.monai.io/en/stable/transforms.html#randcropbyposneglabeld)\n","*   Change the loss function (See https://docs.monai.io/en/stable/losses.html and https://pytorch.org/docs/stable/nn.html#loss-functions )\n","\n","You may edit this jupyter notebook to run all the experiments and plot the results. Considering the time required to train one network, we do not expect any cross-validation or extensive experimentation. You must show **at least three different settings/models** and compare their performance after at least **10 epochs**.\n","\n","- **Hand-In Code**: The full code (.ipynb file) to run the experiment and plot the results. \n","- **Hand-In Answers**: The plots you made and the answer to the question below.\n","\n","3. **Question:** Did you find an improvement with your changes? How can you explain the (lack of) improvement?\n","\n"],"metadata":{"id":"Pqo0-4M_AY_J"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"kNgSlLg56QtW"}}],"metadata":{"accelerator":"GPU","colab":{"name":"spleen_segmentation_3d_student.ipynb","provenance":[{"file_id":"1J7ONpAxt5Cwy3mOFQ01mFDh59kguOoQb","timestamp":1650360153658}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}